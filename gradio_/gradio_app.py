# gradio_app.py
import gradio as gr
import requests
BACKEND_URL = "http://localhost:8000/v1/chat"
BASE_URL = "http://localhost:8000"

def route_query(query: str, user_tier: str):
    """
    è°ƒç”¨ä½ çš„ FastAPI è·¯ç”±ç½‘å…³ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœã€‚
    """
    if not query.strip():
        return "è¯·è¾“å…¥é—®é¢˜", "", "", "", ""

    try:
        payload = {
            "query": query.strip(),
            "user_id": "gradio_demo_user",
            "user_tier": user_tier.lower(),
            "temperature": 0.0,
            "max_tokens": 1000
        }

        response = requests.post(BACKEND_URL, json=payload, timeout=30)

        if response.status_code == 200:
            data = response.json()
            answer = data.get("text", "æ— å›ç­”")
            model_used = data.get("model", "N/A")
            intent = data.get("intent", "unknown")
            cost = f"${data.get('cost', 0):.6f}"
            latency = f"{data.get('latency', 0):.3f} ç§’"

            # æ„å»ºè¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºå±•ç¤ºï¼‰
            details = (
                f"**ä½¿ç”¨æ¨¡å‹**: {model_used}\n"
                f"**è¯†åˆ«æ„å›¾**: {intent}\n"
                f"**è°ƒç”¨æˆæœ¬**: {cost}\n"
                f"**å“åº”å»¶è¿Ÿ**: {latency}"
            )

            return answer, details, model_used, intent, cost
        else:
            error_msg = response.json().get("detail", "æœªçŸ¥é”™è¯¯")
            return f"âŒ è°ƒç”¨å¤±è´¥: {error_msg}", "", "", "", ""

    except requests.exceptions.ConnectionError:
        return "âŒ æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡ï¼Œè¯·ç¡®ä¿ FastAPI æ­£åœ¨è¿è¡Œï¼", "", "", "", ""
    except Exception as e:
        return f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", "", "", "", ""


def route_query_stream(query: str, user_tier: str):
    """
    æµå¼è¯»å–åç«¯ç»“æœå¹¶æ›´æ–° UI
    """
    if not query.strip():
        # è¿”å›é»˜è®¤å ä½ç¬¦ï¼Œä¿æŒè¾“å‡ºé•¿åº¦ä¸ outputs åˆ—è¡¨ä¸€è‡´
        yield "è¯·è¾“å…¥é—®é¢˜", "N/A", "N/A", "N/A", "N/A"
        return

    payload = {
        "query": query.strip(),
        "user_tier": user_tier.lower(),
        "temperature": 0.0,
        "max_tokens": 1000
    }

    try:
        # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯æˆ‘ä»¬ä¸Šä¸€é˜¶æ®µåœ¨ FastAPI ä¸­æ–°å¢çš„ /v1/stream_chat æ¥å£
        with requests.post(f"{BASE_URL}/v1/stream_chat", json=payload, stream=True, timeout=60) as r:
            if r.status_code != 200:
                yield f"âŒ é”™è¯¯: {r.text}", "N/A", "N/A", "N/A", "N/A"
                return

            partial_text = ""
            # è¿™é‡Œçš„é€»è¾‘æ˜¯é€å—è·å– AI è¿”å›çš„æ–‡å­—å¹¶ç«‹å³ yield ç»™ Gradio
            for chunk in r.iter_content(chunk_size=None):
                if chunk:
                    decoded_chunk = chunk.decode("utf-8")
                    partial_text += decoded_chunk
                    # ä¾æ¬¡å¯¹åº” UI ä¸­çš„ outputsï¼šå›ç­”, è·¯ç”±è¯¦æƒ…, æ¨¡å‹, æ„å›¾, æˆæœ¬
                    yield partial_text, "â³ æ­£åœ¨ç”Ÿæˆè¯¦æƒ…...", "æ­£åœ¨ç¡®å®š...", "åˆ†æä¸­...", "è®¡ç®—ä¸­..."

            # ç”Ÿæˆç»“æŸåï¼Œæœ€åå¯ä»¥å‘ä¸€æ¬¡å®Œæ•´çš„çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            yield partial_text, "âœ… ç”Ÿæˆå®Œæˆ", "å·²é”å®š", "å·²é”å®š", "å·²è®¡ç®—"

    except Exception as e:
        yield f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}", "N/A", "N/A", "N/A", "N/A"

# è‡ªå®šä¹‰ CSSï¼ˆå¯é€‰ï¼šè®©ç•Œé¢æ›´ç¾è§‚ï¼‰
custom_css = """
.gradio-container { 
    max-width: 800px !important; 
    margin: auto;
}
#title {
    text-align: center;
    color: #4F46E5;
    font-weight: bold;
}
"""

with gr.Blocks(css=custom_css, title="LLM è·¯ç”±ç½‘å…³") as demo:
    gr.Markdown(
        """
        # ğŸ§  æ™ºèƒ½ LLM è·¯ç”±ç½‘å…³æ¼”ç¤º
        æ ¹æ®**ç”¨æˆ·ç­‰çº§**ä¸**é—®é¢˜æ„å›¾**ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT-4ã€Claudeã€Kimi ç­‰ï¼‰ã€‚
        """,
        elem_id="title"
    )

    with gr.Row():
        with gr.Column(scale=3):
            query_input = gr.Textbox(
                label="ğŸ“ è¾“å…¥ä½ çš„é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼š'å†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾ç®—æ³•' æˆ– 'é«˜è¡€å‹åƒä»€ä¹ˆè¯ï¼Ÿ'",
                lines=3
            )
            tier_input = gr.Radio(
                choices=["free", "basic", "premium"],
                value="premium",
                label="ğŸ‘¤ ç”¨æˆ·ç­‰çº§"
            )
            submit_btn = gr.Button("ğŸš€ å‘é€æŸ¥è¯¢", variant="primary")

        with gr.Column(scale=2):
            model_badge = gr.Textbox(label="ğŸ¯ ä½¿ç”¨çš„æ¨¡å‹", interactive=False)
            intent_badge = gr.Textbox(label="ğŸ” è¯†åˆ«æ„å›¾", interactive=False)
            cost_badge = gr.Textbox(label="ğŸ’° æˆæœ¬", interactive=False)

    answer_output = gr.Textbox(label="ğŸ’¬ æ¨¡å‹å›ç­”", interactive=False, lines=6)
    details_output = gr.Markdown(label="ğŸ“Š è·¯ç”±è¯¦æƒ…")

    # ç»‘å®šäº‹ä»¶
    submit_btn.click(
        fn=route_query_stream,
        inputs=[query_input, tier_input],
        outputs=[answer_output, details_output, model_badge, intent_badge, cost_badge]
    )

    # æ”¯æŒå›è½¦å‘é€
    query_input.submit(
        fn=route_query_stream,
        inputs=[query_input, tier_input],
        outputs=[answer_output, details_output, model_badge, intent_badge, cost_badge]
    )

    gr.Markdown(
        """
        ---
        ğŸ’¡ **æç¤º**ï¼š  
        - **Free ç”¨æˆ·** å¯èƒ½è¢«è·¯ç”±åˆ°ä½æˆæœ¬æ¨¡å‹ï¼ˆå¦‚ DeepSeekï¼‰  
        - **Premium ç”¨æˆ·** ä¼˜å…ˆä½¿ç”¨é«˜æ€§èƒ½æ¨¡å‹ï¼ˆå¦‚ GPT-4ã€Claude 3.5ï¼‰  
        - ç³»ç»Ÿä¼šæ ¹æ®é—®é¢˜ç±»å‹ï¼ˆä»£ç ã€åŒ»ç–—ã€é€šç”¨ç­‰ï¼‰æ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹
        """
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å±€åŸŸç½‘è®¿é—®ï¼ˆå¯é€‰ï¼‰
        server_port=6006,  # é»˜è®¤ç«¯å£
        share=False
    )